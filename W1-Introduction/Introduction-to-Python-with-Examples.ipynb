{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Python Basics\n",
    "\n",
    "* Shift/enter runs a command in a cell\n",
    "* If this doesn't work, try Kernel/Restart then shift/enter\n",
    "* Cell/All output/Clear clears the results from the notebook\n",
    "* The command pallette (small rectangle above) lets you specify the type of a cell\n",
    "\n",
    "* In MyBinder, you can edit a notebook but the notebook restores to defaults upon exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.getcwd() # what is your current work directory? \n",
    "os.chdir(\"C:\\\\Users\\\\Administrator\\\\Desktop\") # change your working directory (enter correct path). Note backslash direction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data types in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2+2) # integer \n",
    "\n",
    "print(8/2) # floating-point numbers\n",
    "type(8/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Text analysis is pretty cool!\"\n",
    "type(text) # string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists are mutable vectors []\n",
    "\n",
    "\n",
    "list_example = [12, \"hungry\", \"dogs\"]\n",
    "type(list_example) # list\n",
    "print(list_example[2]) # the index starts from 0\n",
    "print(list_example[0:2]) # slice the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuples are immutable ()\n",
    "\n",
    "tuple_example = (12, \"hungry\", \"dogs\") # note the difference between () and []\n",
    "len(tuple_example)\n",
    "type(tuple_example) # tuple \n",
    "print(tuple_example(2)) # the error says the tuple object is not callable; tuple is immutable while list is mutable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries are lists with keys and values {}\n",
    "\n",
    "dict_example = {\"a\": 1, \"b\" : 42, \"text\" : \"hi there\"}\n",
    "type(dict_example) # dict\n",
    "print(dict_example[\"text\"]) # we can use key to get what we want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Method is a command that is specific to an object type. \n",
    "#Here, .insert and .append are list methods.\n",
    "\n",
    "text_list = [2, 5, \"yes\"]\n",
    "text_list.insert(0, \"no\") # insert \"no\" to the location 0\n",
    "print(text_list) \n",
    "\n",
    "text_list.append(\"whatever\") # append \"whatever\" at the end of the list\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops are probably familiar. They are very useful when dealing with thousands of repetitive tasks\n",
    "\n",
    "test_list = [\"Dr.Slater\", \"Dr.Pepinsky\", \"Lily\", \"Samantha\"]\n",
    "\n",
    "for element in test_list:\n",
    "    if \"Dr.\" in element:\n",
    "        print(\"Hello \" + element + \"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a loop to apply a method to a list 5 times \n",
    "myList=[]\n",
    "for element in range(5):\n",
    "    myList.append(element) # append is a method or attribute of the \"list\" object (or module)\n",
    "print(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function is a block of organized, reusable code used to perform a single, related action.\n",
    "#Here, the function 'perfect' is applied to whatever values are assigned to the variable 'score'\n",
    "\n",
    "\n",
    "def perfect(score):\n",
    "    print (\"I got a perfect \" + score)\n",
    "    \n",
    "perfect(score='100') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinfo(name, age):\n",
    "   print(\"Name:\", name)\n",
    "   print(\"Age:\", age)\n",
    "   \n",
    "pinfo(age=25, name=\"Joanne\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A sequence of characters that define a search pattern. For example, '\\,' says look for a comma. See https://docs.python.org/3.4/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Regular expression is a special sequence of characters for finding strings in text\n",
    "#They are incredibly useful for finding and extracting text\n",
    "# See https://docs.python.org/3/library/re.html\n",
    "\n",
    "#For example, let's split 'happy, go lucky' wherever there is a comma, and whereever there is a space\n",
    "\n",
    "from bs4 import re\n",
    "re.split('\\,', 'happy, go lucky') \n",
    "#split wherever there is a comma (the backslash says - don't treat comma as an RE character)\n",
    "\n",
    "re.split('\\s', 'happy, go lucky')\n",
    "#split whereever there is a space (in this case \\s is an RE for space)\n",
    "\n",
    "#try some regular expressions here  http://www.regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You write a script to scrape addresses from thousands of records. \n",
    "#The field you are scraping does not exist in one of the records so your program crashes. \n",
    "#The ignore and try/except commands say in effect: if it works do it, if not ignore and move on to the next case!\n",
    "#Here's an example that is part of a function that let's you know when it happens\n",
    "\n",
    "def divide(x, y):\n",
    "    try:\n",
    "        result = x / y\n",
    "    except ZeroDivisionError:\n",
    "        print(\"division by zero!\")\n",
    "    else:\n",
    "        print(\"result is\", result)\n",
    "\n",
    "\n",
    "divide(2.0,3.0)\n",
    "divide(2.0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you just want to ignore bad cases (rather than printing an error message):\n",
    "\n",
    "def divide(x, y):\n",
    "    try:\n",
    "        result = x / y\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"result is\", result)\n",
    "\n",
    "divide(2.0,3.0)\n",
    "divide(2.0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to remove unicode from files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The characters you see in text are based on an underlying encoding. \n",
    "Ascii is the common encoding but some texts have unicode encoded characters\n",
    "The best way to deal with the problems they create is to remove them in advance\n",
    "\n",
    "This script converts the text to strings that are sentences in ascii format. \n",
    "#If a character can't be converted to ascii (e.g. some unicode characters), ignore it \n",
    "cleanwords=str(sentence.encode('ascii',errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script converts the text to strings that are sentences in ascii format. \n",
    "# If a character can't be converted to ascii (e.g. some unicode characters), ignore it \n",
    "\n",
    "cleanwords=str(sentence.encode('ascii',errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a more deliberate way to remove unicode from a list of files.\n",
    "# This worked when the above ignore command did not \n",
    "\n",
    "list2 = []\n",
    "file_counter = 0\n",
    "for file in list1:\n",
    "    file_counter += 1\n",
    "    missing_words = 0\n",
    "    out_file = ''\n",
    "    word_list = file.split()\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            new_word = str(word)\n",
    "            out_file = '%s %s'%(out_file, new_word)\n",
    "        except:\n",
    "            missing_words += 1\n",
    "    list2.append(out_file)\n",
    "    print('%s%s%s%s'%('File: ', file_counter, '| Missing words: ', missing_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Part II: Collecting and Pre-processing Text \n",
    "\n",
    "* Scraping (two examples using an API or scraping directly from a website)\n",
    "* Splitting documents\n",
    "* Tokenizing and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data using API\n",
    "* This [tutorial](https://dlab.berkeley.edu/blog/scraping-new-york-times-articles-python-tutorial) demonstrates how to use the New York Times Articles Search API using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a single website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "polisci_url = urlopen('https://www.polisci.washington.edu/people')\n",
    "type(polisci_url) \n",
    "polisci_page = polisci_url.read()\n",
    "\n",
    "print(polisci_page)\n",
    "type(polisci_page) # When the content contains an unrecognized character Python will convert it into bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix that by using the correct character encoding from `Content-Type` request header\n",
    "charset_encoding = polisci_url.info().get_content_charset()\n",
    "# apply encoding\n",
    "polisci_page = polisci_url.read().decode(charset_encoding)\n",
    "\n",
    "type(polisci_page) # Now it is a long string instead of a byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handier way to fix possible encoding issues\n",
    "polisci_page = urlopen('https://www.polisci.washington.edu/people').read().decode('utf-8')\n",
    "print(polisci_page)\n",
    "type(polisci_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This says create a new list polisci_list from the polisci_page list that splits at every space\n",
    "polisci_list = [polisci_page.split()]\n",
    "polisci_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make the text more readable?\n",
    "\n",
    "*Ok that list is kind of a mess because it contains a lot of html code that we don't care about. \n",
    "Fortunately people have written programs to remove much of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, re\n",
    "\n",
    "polisci_url = urlopen(\"https://www.polisci.washington.edu/people\")\n",
    "polisci_page = BeautifulSoup(polisci_url.read())\n",
    "polisci_text = polisci_page.get_text()\n",
    "print(polisci_text)\n",
    "type(polisci_text) # string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is almost always the case that additional cleanup steps are required. For example...\n",
    "#remove all of the blank lines \n",
    "\n",
    "lines = (line.strip() for line in polisci_text.splitlines())\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "politext = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "print(politext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we might want to extract specific things from a text using a regular expression!\n",
    "#Here the RE finds all of the strings corresponding to a particular pattern of characters (an email address)\n",
    "\n",
    "polisci_emails = set()\n",
    "emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", politext, re.I)) #https://docs.python.org/3/library/re.html\n",
    "polisci_emails.update(emails)\n",
    "\n",
    "print(polisci_emails)\n",
    "type(polisci_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the set of emails above to a list \n",
    "email_list = list(polisci_emails)\n",
    "print(email_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse names and positions and save them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# collect and extract specific information from a web page using html fields.\n",
    "#to see these fields, go to the website, right click and 'view page source'\n",
    "\n",
    "r = requests.get(\"https://www.seattle.gov/elected-officials\") \n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "boxes = soup.find_all('div', class_ = 'primaryContent')\n",
    "\n",
    "name_item = [box.h3.text for box in boxes]  #looks for text that is boxed by h3\n",
    "position_item = [box.span.text for box in boxes]  #looks for text that is boxed by span\n",
    "    \n",
    "# for better presentation (look for seattle_officials.csv in your working directory?)\n",
    "import pandas\n",
    "pandas.DataFrame({'position':position_item, 'name':name_item}).to_csv('seattle_officials.csv', index = False)\n",
    "\n",
    "print(name_item)\n",
    "print(position_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the timeline of US-Iran relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://pri.org/stories/2020-01-03/history-us-iran-relations-timeline')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "header_tags = soup.find_all('h3')  \n",
    "headers = [h.text.strip() for h in header_tags]  #text.strip removes the surrounding html code whereever an h3 field is found\n",
    "\n",
    "print(header_tags)\n",
    "print(headers)\n",
    "\n",
    "dates = [header.split(': ')[0] for header in headers]  #split what's in the h3 box by : and first item is date\n",
    "events = [header.split(': ')[1] for header in headers] #second item is event\n",
    "descriptions = [h.next_sibling.text.strip() for h in header_tags] \n",
    "#next_sibling says grab the next item at the same html level after each header_tag. Shows how much you can do!\n",
    "\n",
    "print(dates)\n",
    "print(events)\n",
    "print(descriptions)\n",
    "\n",
    "import pandas\n",
    "pandas.DataFrame({'date':dates, 'event': events, 'description':descriptions}).to_csv('iran_history.csv', index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all('span', attrs={'class':'short-desc'})\n",
    "\n",
    "records = []\n",
    "for result in results:\n",
    "    date = result.find('strong').text[0:-1] + ', 2017'  #find text in bold font and add '2017' to it\n",
    "    lie = result.contents[1][1:-2] \n",
    "    explanation = result.find('a').text[1:-1]\n",
    "    url = result.find('a')['href']\n",
    "    records.append((date, lie, explanation, url))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(records, columns=['date', 'lie', 'explanation', 'url'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.to_csv('trump_lies.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
